{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **End-to-End neural ad-hoc ranking with kernel pooling**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура `K-NRM` (_Kernel Neural Ranking Model_) похожа на `DRMM`, здесь так же учитываются взаимосвязи _каждый-с-каждым_ относительно _слов запроса_ и _документа_. В качестве меры близости используется _cosine similarity_, однако полученная матрица сворачивается с помощью ядер, или кернелов (kernels).\n",
    "$$\n",
    "    cosine\\ similarity = S_C(A, B) := cos(\\theta) = \\frac{\\bold{A} * \\bold{B}} {||\\bold{A}||||\\bold{B}||}\n",
    "        = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n}{A_i^2}} \\sqrt{\\sum_{i=1}^{n}{B_i^2}}}\n",
    "$$\n",
    "$$\n",
    "    M_{i j}=\\cos \\left(\\vec{v}_{t_{i}^{q}}, \\vec{v}_{t_{j}^{d}}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Ядро представляет собой `RBF-kernel` (_Radial Basis Function_), который вам может быть знаком из машинного обучения. Каждое такое ядро — это функция от целой строки в матрице взаимодействий $M_{ij}$ между запросом и документом, или, иными словами, взаимодействие одного слова из запроса со всеми словами документа за раз. Это ядро имеет два параметра, которые не обучаются, а фиксируются при инициализации: среднее $\\mu_k$ и параметр дисперсии $\\sigma_k$, который стоит в знаменателе. Эта сигма обычно одинакова для всех ядер (кернелов).\n",
    "\n",
    "$$\n",
    "    K_{k}\\left(M_{i}\\right)=\\sum_{j} \\exp \\left(-\\frac{\\left(M_{i j}-\\mu_{k}\\right)^{2}}{2 \\sigma_{k}^{2}}\\right)\n",
    "$$\n",
    "\n",
    "Давайте разберёмся, как эта формула работает и в чём её смысл. За счёт среднего, параметра $\\mu_k$, который уникален для каждого ядра, происходит привязка к конкретному значению в исходной матрице схожести, т.е. к значению _similarity_, с которым сравнивается каждый элемент матрицы $M_{ij}$ попарных взаимодействий эмбеддингов слов запроса и документа. Каждое ядро можно представить как волну, где по оси $OY$ отмечен вес, с которым объект, или токен в документе, входит в конкретную область.\n",
    "\n",
    "![https://storage.yandexcloud.net/klms-public/production/learning-content/3/10/162/539/3637/image.png](https://storage.yandexcloud.net/klms-public/production/learning-content/3/10/162/539/3637/image.png)\n",
    "\n",
    "Рассмотрим пример на картинке выше — в нижней части синими точками отмечены документы. Чем левее точка, тем меньше, вплоть до $-1$, cosine similarity между словом в документе, которое представлено точкой, и конкретным зафиксированным словом в запросе. Серая точка вверху символизирует выходное значение конкретного ядра. Видно, что второй кернел имеет наибольшее значение. Это обусловлено тем, что близко к его центру находится целых два объекта, т.е. они наиболее похожи именно на центр волны, которая описывает второе ядро. Далее идёт кернел с номером один, у которого тоже целых две точки, однако они чуть дальше от центра кернела $K1$, нежели такие точки у кернела $K2$. У третьего кернела совсем всё грустно — точки далеки от его центра, а значит, в это ядро практически ничего не попадает и его значение минимально. По сути это сглаженный и дифференцируемый аналог бинов, на которые разбивали отрезок $[-1, 1]$ — ось cosine similarity при расчёте гистограмм.\n",
    "\n",
    "Для того чтобы симулировать несколько бинов, т.е. оценивать плотность распределения метрики косинусной схожести слов запроса и документа в разных частях интервала, берётся несколько ядер с некоторым шагом между их средним, т.е. $\\mu_k$. Допустим, для отрезка $[-1, 1]$ будет $11$ ядер, у первого cреднее в $1$, далее $0.9$, $0.7$ и так далее до $-0.9$. Тогда каждое из ядер даст скалярное числовое значение на выходе, характеризующее схожесть запроса и документа в некоторой окрестности определенной cosine similarity, и объединив все ядра для описания полной картины можно получить вектор. В случае с $11$ ядрами, или кернелами, получаем вектор из $11$ признаков, который несёт в себе ту же информацию, что и Matching Histogram в DRMM. Собственно, такой вектор и получится $\\vec{K}\\left(M_{i}\\right)=\\left\\{K_{1}\\left(M_{i}\\right), \\ldots, K_{K}\\left(M_{i}\\right)\\right\\}$\n",
    "\n",
    "Ещё раз обращаем внимание на то, что значение ядра тем больше, чем кучнее и ближе к среднему значению $\\mu_k$ конкретно этого ядра находятся результирующие косинусные схожести всех слов документа и конкретного слова запроса (аналогия с бинами в DRMM).\n",
    "\n",
    "Для полноты картины осталось осознать, на что влияет параметр $\\sigma_k$, т.е. дисперсия. По сути она задаёт ширину бина, в который попадают слова из документа. На изображении ниже показаны различные параметры $\\sigma_k$ для одного кернела со средним, т.е. параметром $\\mu_k$, равным $0.7$. Чем больше $\\sigma_k$, тем шире бин и тем на большую окрестность обращает внимание этот кернел. Под каждой из $\\sigma_k$ справа вынесено изменение метрики MAP (Mean Average Precision) на некотором наборе данных. Это демонстрирует, что подбор $\\sigma_k$  для конкретного набора данных в рамках вашей задачи — важная процедура, так как ширина бина влияет на качество модели. Вообще такие ядра можно назвать сглаженной версией TF (Term Frequency), или SOFT-TF, поскольку логика работы с бинами сохраняется и происходит только нестрогое присваивание конкретного слова из документа определённому бину. При этом само слово учитывается в каждом бине, но в большинстве из них с околонулевым весом. По оси $OY$ на графике отражён вес, с которым объект войдёт в сумму: чем дальше от центра ядра, тем меньше вес. Если все объекты находятся далеко от кернела, то, с одной стороны, можно сказать, что этот бин пустой и в методе DRMM было бы записано 0, а с другой — в soft-версии у нас сумма околонулевых значений, т.е. величина незначительная. И наоборот, если в бине или в центре кернела объектов много, то и значение активации на этом ядре будет высоким.\n",
    "\n",
    "![Демонстрация поведения формы ядра для различных значениях дисперсии](https://storage.yandexcloud.net/klms-public/production/learning-content/3/10/162/539/3637/image_Nd2X53w.png)\n",
    "\n",
    "Демонстрация поведения формы ядра для различных значениях дисперсии\n",
    "\n",
    "Когда данная операция применена ко всем строчкам таблицы, на выходе получается $N$ векторов размера $K$, где $K$ — количество ядер. Всё, что осталось сделать — сначала просуммировать по $N$, т.е. по всем словам из запроса, логарифмы значений, полученных на ядрах $\\varphi(M)=\\sum_{i=1}^{n} \\log \\vec{K}\\left(M_{i}\\right)$, таким образом собрав их в единый вектор размера $K$, а затем простым преобразованием вроде $f(q, d)=\\tanh \\left(w^{T} \\varphi(M)+b\\right)$ получить выходное значение релевантности. Тут имеем вектор весов $w$ и смещение $b$, это обучаемые параметры.\n",
    "\n",
    "Легко заметить, что если убрать эмбеддинги из модели, то вообще у модели обучаемых параметров всего 12 (это очень мало): 11 в ядрах и 1 для смещения! Вместо многомиллионных моделей обучается всего 12 параметров. Может показаться странным, но в K-NRM из-за замены жестких бинов на сглаженные версии на кернелы появляется возможность дифференцировать весь алгоритм, считать градиенты и прокидывать их на входные эмбеддинги, по которым оценивается косинусная схожесть. Это даёт удивительную возможность дообучать эмбеддинги слов под свою задачу, т.е. в прямом смысле переопределять значения слов и словосочетаний исходя из потребностей. При этом никто не мешает инициализировать эмбеддинги уже готовыми обученными параметрами, скачанными из интернета, и просто дообучать модель под свои нужды. Это позволяет невероятно быстро обучать модель K-NRM под свои данные, так как практически все веса у нас хорошо инициализированы, и нужно их лишь слегка изменить.\n",
    "\n",
    "![Архитектура K-NRM](https://storage.yandexcloud.net/klms-public/production/learning-content/3/10/162/539/3637/image_YXBoS8M.png)\n",
    "\n",
    "Архитектура K-NRM\n",
    "\n",
    "В практике ML это называется _`Transfer Learning`_, когда знания с одной задачи, в данном случае общеязыкового моделирования, переносятся на другую. В применении, т.е. при инференсе эта модель крайне быстра, потому что тут из операций самое сложное — расчёт матрицы _cosine similarity_ от _каждого-к-каждому_ на входных эмбеддингах. Все остальные операции не требуют затрат и очень просты."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем архитектуру `KNRM`, подготовим данные для обучения и напишем пайплайн тренировки модели.\n",
    "\n",
    "В качестве датасета будет использоваться набор из пар вопросов с сайта `Quora`, где указано, является ли один из вопросов дубликатом другого. Это задача бинарной классификации. В качестве кандидатов брались максимально похожие вопросы, после чего производилась разметка. В рамках работы по ранжированию будет решаться не задача определения дубликатов, а задача нахождения максимально похожих (т.е. релевантных) вопросов, которые могут удовлетворить пользователя ещё на этапе формулировки. Это можно рассматривать как _suggest_-систему, расположенную под плашкой _\"возможно, эти вопросы помогут вам\"_. Поэтому введём три уровня релевантности:\n",
    "* `2` — вопрос является полным дубликатом (согласно оригинальной разметке, это пары с таргетом, равным 1)\n",
    "* `1` — вопрос очень похож на исходный, однако не является полным дубликатом (согласно оригинальной разметке, это пары с таргетом, равным 0)\n",
    "* `0` — вопрос не похож на исходный, нерелевантный (таких пар в датасете нет, их можно нагенерировать самостоятельно из общего корпуса всех вопросов)\n",
    "\n",
    "Этот датасет _Quora Question Pairs_ (`QQP`) входит в набор датасетов GLUE, используемый для всесторонней оценки моделей машинного обучения, связанных с текстом. Скачать его можно [на сайте](https://gluebenchmark.com/tasks/) или непосредственно [по ссылке](https://dl.fbaipublicfiles.com/glue/data/QQP-clean.zip). \n",
    "\n",
    "В архиве лежат три файла: \n",
    "* `train` используется для тренировки модели;\n",
    "* `dev` применяется для оценки в рамках домашней работы;\n",
    "* `test` на данном этапе не используется. \n",
    "\n",
    "В качестве текстовых эмбеддингов будут использоваться вектора `GloVe 6B`. Ознакомиться с особенностями этих эмбеддингов вы можете [на официальном сайте](https://nlp.stanford.edu/projects/glove/), а вот точная [ссылка](http://nlp.stanford.edu/data/glove.6B.zip) на скачивание. В архиве представлены вектора разной размерности, однако для проверки работоспособности и оценки решения будут использоваться исключительно вектора размерности 50 (файл glove.6B.50d.txt внутри архива). Понятно, что можно получить более высокое качество (вероятно, незначительно более высокое), используя эмбеддинги размерности 300. Однако это накладывает ограничения на ресурсы (память на диске и ОЗУ), время работы алгоритма и т.д."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Детали реализации и описание"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `min_token_occurancies` — минимальное количество раз, которое слово (_токен_) должно появиться в выборке, чтобы не быть отброшенным как _низкочастотное_. При значении, равном единице, остаются все слова, которые представлены в датасете.\n",
    "\n",
    "* `emb_rand_uni_bound` — половина ширины интервала, из которого равномерно (_uniform_) генерируются вектора эмбеддингов (если вектор не представлен в наборе _GloVe_). Если параметр равен $0.2$, то каждая компонента вектора принадлежит $U(−0.2,0.2)$.\n",
    "\n",
    "* `freeze_knrm_embeddings` — флаг, указывающий на необходимость дообучения эмбеддингов, будут ли по ним считаться градиенты (при `True` дообучение происходить не будет).\n",
    "\n",
    "* `knrm_kernel_num` — количество ядер в `KNRM`.\n",
    "\n",
    "* `knrm_out_mlp` — конфигурация _MLP_-слоя на выходе в `KNRM`.\n",
    "\n",
    "* `dataloader_bs` — размер батча при обучении и валидации модели.\n",
    "\n",
    "* `train_lr` — _Learning Rate_, использующийся при обучении модели `KNRM`.\n",
    "\n",
    "* `change_train_loader_ep` — как часто менять/перегенерировать выборку для тренировки модели."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 1. Реализация токенизации и препроцессинга данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Необходимо реализовать методы: \n",
    "\n",
    "* `handle_punctuation` — очищает строку от пунктуации. Все знаки пунктуации необходимо взять из `string.punctuation`.\n",
    "\n",
    "* `simple_preproc` — полный препроцессинг строки. Включает в себя обработку пунктуации и приведение к нижнему регистру, а в качестве токенизации (_разбиения предложения на слова или их усеченную версию — токены_) использован метод `nltk.word_tokenize` из библиотеки `nltk`. На выходе — лист со строками (токенами).\n",
    "\n",
    "* `get_all_tokens` — метод, формирующий список **ВСЕХ** токенов, представленных в подаваемых на вход датасетах (`pd.DataFrame`). Для реализации необходимо сформировать уникальное множество всех текстов, затем рассчитать частотность каждого токена (т.е. после обработки `simple_preproc`) и отсечь те, которые не проходят порог, равный `min_token_occurancies`, с помощью метода `_filter_rare_words`. На выходе — список токенов, для которых будут формироваться эмбеддинги и на которые будут разбиваться оригинальные тексты вопросов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 2. Создание матрицы эмбеддингов и словаря токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Необходимо реализовать методы: \n",
    "\n",
    "* `_read_glove_embeddings` — считывание файла эмбеддингов в словарь, где ключ — это слово, а значение — это вектор эмбеддинга (можно не приводить к `float`-значениям).\n",
    "\n",
    "* `create_glove_emb_from_file` — метод формирует `matrix` (матрица эмбеддингов размера $N∗D$, где $N$ — количество токенов, $D$ — размерность эмбеддинга), `vocab` (словарь размера $N$, сопоставляющий каждому слову индекс эмбеддинга), `unk_words` — список слов, которые не были в исходных эмбеддингах, и потому для них пришлось генерировать случайный эмбеддинг из равномерного распределения (или другой вектор с заданными характеристиками).\n",
    "\n",
    "**Обратите внимание:** необходимо в словарь добавить два специальных токена — _`PAD`_ и _`OOV`_, с индексами $0$ и $1$ соответственно. Первый токен используется для заполнения пустот в тензорах (когда один вопрос состоит из бОльшего количества токенов, чем второй, однако их необходимо представить в виде матрицы, в которой строки имеют одинаковую длину) и должен состоять полностью из нулей. Второй токен используется для токенов, которых нет в словаре. К примеру, такое может встретиться в новых текстах: если бы `all_tokens` формировались исключительно по тренировочным данным, то в `dev`-датасете были бы слова, которые модель не видела. Такие слова принято заменять на `OutOfVocab` (_OOV_) токены. Пример вызова этого метода и передаваемых аргументов можете посмотреть в методе `build_knrm_model`. На выходе в матрице эмбеддингов (и в словаре) должны быть как загруженные из файла вектора (для тех слов, которые в нём встретились), так и вектора для новых слов (из `unk_words`, включая `PAD`  и `OOV`). \n",
    "\n",
    "\n",
    "> Можете проверить себя на этом этапе: для `min_token_occurancies=1` доля `unk_words` из всех слов должна быть около $30\\%$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 3. Имплементация `Kernels` и модели `KNRM`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`GaussianKernel` не содержит обучаемых параметров и служит простым нелинейным оператором — необходимо перевести формулу в метод `forward` класса. Параметр $\\mu$ отвечает за \"среднее\" ядра, точку внимания, $\\sigma$ — за ширину “бина”.\n",
    "\n",
    "`KNRM` — класс, характеризующий всю сеть. В нём нужно имплементировать:\n",
    "\n",
    "* `_get_kernels_layers` — формирует список всех ядер ($K$ штук), применяемых в алгоритме. Важно обратить внимание на автоматическую генерацию $\\mu$ для каждого ядра, а также на крайнее правое значение. К примеру, если $K=5$, то $\\mu$ должны быть $[-0.75, -0.25, 0.25, 0.75, 1]$, а для $K=11$ $[-0.9, -0.7, -0.5, -0.3, -0.1, 0.1, 0.3, 0.5, 0.7, 0.9, 1]$. Для понимания принципа генерации проанализируйте симметрию относительно нуля, размер интервалов и краевые значения. Параметр `exact_sigma` означает $\\sigma$-значение для крайнего бина, в котором $\\mu$ равняется единице.\n",
    "\n",
    "* `_get_mlp` — формирует выходной _MLP_-слой для ранжирования на основе результата `Kernels`. Точная структура зависит от атрибута `out_layers`. Если `out_layers = []`, то _MLP_ становится линейным слоем из $K$ (признаки равны результатам ядер) в $1$ (финальный скор релевантности). Если `out_layers = [10, 5]`, то архитектура будет следующая: \n",
    "    > $K \\rightarrow ReLU \\rightarrow 10 \\rightarrow ReLU \\rightarrow 5 \\rightarrow ReLU \\rightarrow 1$\n",
    "\n",
    "Обратите внимание, что нелинейность не применяется в конце _MLP_. Таким образом, с помощью цикла нужно научиться в автоматическом режиме генерировать архитектуру выходного слоя. Детальнее о концепции _MLP_ можно узнать в Википедии. В качестве нелинейности, как указано выше, предлагается использовать `ReLU`.\n",
    "\n",
    "Методы `forward` и `predict` уже реализованы. Обратите внимание, что KNRM будет обучаться в PairWise-режиме. Для этого необходимо соответственно подготовить данные.\n",
    "\n",
    "**Функции непосредственного расчёта модели:**\n",
    "\n",
    "* `_get_matching_matrix` — формирует матрицу взаимодействия “каждый-с-каждым” между словами одного и второго вопроса (запрос и документ). В качестве меры используется косинусная схожесть (_cosine similarity_) между эмбеддингами отдельных токенов:\n",
    "$$\n",
    "    cosine\\ similarity = S_C(A, B) := cos(\\theta) = \\frac{\\bold{A} * \\bold{B}} {||\\bold{A}||||\\bold{B}||}\n",
    "        = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n}{A_i^2}} \\sqrt{\\sum_{i=1}^{n}{B_i^2}}}\n",
    "$$\n",
    "\n",
    "* `_apply_kernels` — применяет ядра к `matching_matrix`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 4. Подготовка Datasets и Dataloaders для обучения и валидации модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Dataset` и `Dataloader` — важные части пайплайнов обучения на _PyTorch_. По сути это умные и гибкие обёртки над данными, которые позволяют итерироваться по набору данных. Реализуем два `Dataset` — для _тренировки_ и _валидации_. У них есть существенное отличие — первый работает с триплетами документов (исходный вопрос, вопрос-кандидат1 и вопрос-кандидат2 для обучения в _PairWise_-режиме), второй работает с парами (оценивает отдельно релевантность вопроса-кандидата к исходному вопросу). Метод генерации пар для валидации уже был упомянут выше — `create_val_pairs`.\n",
    "\n",
    "Для этих датаcетов есть общий класс, который отвечает за обработку текстов — `RankingDataset`.\n",
    "\n",
    "* `idx_to_text_mapping` отвечает за соотнесение индекса `(id_left и id_right)` с текстом, подробнее в методе `get_idx_to_text_mapping` класса `Solution`.\n",
    "\n",
    "* `vocab` — маппинг слова в индекс (ведь именно индексы слов подаются в эмбеддинг-слой KNRM в качестве входов, и по ним берётся нужная строка матрицы).\n",
    "\n",
    "* `oov_val` — значение (индекс) в словаре на случай, если слово не представлено в словаре.\n",
    "\n",
    "* `preproc_func` — функция обработки и токенизации текста. В примере с созданием `ValPairsDataset` в _init_-методе класса `Solution` видно, что это та же самая функция, которая уже реализована. \n",
    "\n",
    "* `max_len` — максимальное количество токенов в тексте.\n",
    "\n",
    "Итого концептуально `RankingDataset` делает следующее: \n",
    "\n",
    "* `__getitem__` возвращает набор признаков для заданной пары или триплета несколько _id_ и _таргет_, где признаки выражены индексами слов в словаре. Сам метод `__getitem__` в классе `RankingDataset` реализовывать не нужно. Однако нужно сделать `_convert_text_idx_to_token_idxs`, который переведёт `id_left/id_right` в индексы токенов в словаре, в частности с помощью функции `_tokenized_text_to_index` (перевод обработанного текста после `preproc_func` в индексы).\n",
    "\n",
    "Метод `__getitem__` нужно реализовать в наследниках класса — `TrainTripletsDataset` и `ValPairsDataset`, с той лишь разницей, что для тренировки используются тройки документов, а для валидации пары. Сами пары и тройки должны поступать на вход в `index_pairs_or_triplets`. Это список списков _id_ (и конечно, _лейблов_), пример формирования для валидации которого приведён в `create_val_pairs`.\n",
    "\n",
    "На выходе этого метода ожидается один или два словаря с ключами _query_ и _document_, а также целевая метка (для тренировки — ответ на вопрос, действительно ли первый документ более релевантен запросу, чем второй, для валидации — релевантность от 0 до 2).\n",
    "\n",
    "Функция `collate_fn` уже написана, её цель — собирать батч из нескольких тренировочных примеров для `KNRM`. Она принимает на вход список из выходов датасетов выше и формирует из них единый `dict` с тензорами в качестве значений. Можете использовать эту функцию в качестве проверки на то, что датасеты реализованы верно. Эта функция должна передаваться, среди прочего, в `DataLoader` с той целью, чтобы автоматически собирать, к примеру, $128$ объектов (триплетов при обучении) в один набор данных, который будет подан в `KNRM` (см. пример в методе `valid`). Более подробно прочитать можно по [ссылке](https://pytorch.org/docs/stable/data.html#dataloader-collate-fn) в документации."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок 5. Тренировка модели "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Теперь осталось соединить всё вместе, чтобы обучать модель!\n",
    "\n",
    "Реализуйте метод `train`, который совершает $N$ итераций по тренировочному `Dataloader` ($N$ эпох). В зависимости от метода формирования тренировочной выборки при необходимости пересоздавайте выборку каждые `change_train_loader_ep` эпох. Сама тренировочная выборка создаётся в методе `sample_data_for_train_iter`. Вам предлагается самостоятельно придумать и реализовать методику подбора триплетов документов для обучения в _PairWise_-режиме нейросети `KNRM`. В качестве примера можете использовать метод создания валидационного пулла.\n",
    "\n",
    "Так, авторское решение генерирует порядка $8-10$ тысяч триплетов для обучения. Это недетерминированный процесс (без задания `random_seed`), и потому появляется возможность каждые $K$ эпох менять выборку (не меняя смысла задачи — всё еще нужно определять, релевантнее ли второй документ к данному запросу, чем первый), например — менять _left_ и _right_ _id_. Во время обучения в конце эпохи можете воспользоваться методом `valid` для расчёта $nDCG$. Вам необходимо преодолеть порог в $0.925$. Правильное решение даже без смены набора триплетов за $8-12$ эпох (т.е. за $8-12$ итераций по датасету размера $8-10$ тысяч) с `batch_size` $1024$ (т.е. порядка $100$ обновлений весов) способно преодолеть такой порог (**без дообучения эмбеддингов**). Модель при отправке тренируется с нуля до $20$ эпох, но **не более** $7$ минут.\n",
    "\n",
    "Домашнее задание можно выполнять сверху вниз по частям (главное закомментировать неработающие куски кода: например, чтобы отрабатывал метод _init_ класса `Solution`), сверяясь с тем, что всё сделано без ошибок. Блок 4 не проверяется отдельно — только вместе с тренировкой модели (так как есть простор и свобода для логики формирования датасета). Корректность работы возвращаемых значений можно проверить локально с помошью `DataLoader` и `collate_fn`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пути к загруженным данным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_QQP_DIR = './data/QQP/'\n",
    "GLOVE_PATH = './data/glove.6B.50d.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQP data already downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('data/QQP/'):\n",
    "    os.system(\"bash load_data.sh\")\n",
    "else:\n",
    "    print(\"QQP data already downloaded successfully!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.basename(GLOVE_PATH) in os.listdir('./data'):\n",
    "    os.system(\"bash load_embeddings.sh\")\n",
    "else:\n",
    "    print(\"Embeddings already loaded successfully!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Union, Callable, Optional\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GaussianKernel` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernel(torch.nn.Module):\n",
    "    def __init__(self, mu: float = 1.0, sigma: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return torch.exp(-(x - self.mu) ** 2 / (2 * self.sigma ** 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `KNRM` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNRM(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_matrix: np.ndarray,\n",
    "        freeze_embeddings: bool,\n",
    "        kernel_num: int = 21,\n",
    "        sigma: float = 0.1,\n",
    "        exact_sigma: float = 0.001,\n",
    "        out_layers: List[int] = [10, 5]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(embedding_matrix),\n",
    "            # Update embeddings in learning process \n",
    "            freeze=freeze_embeddings,\n",
    "            # If specified, the entries at idx is not updated during training\n",
    "            padding_idx=0\n",
    "        )\n",
    "        # K number of kernels\n",
    "        self.kernel_num = kernel_num\n",
    "        # Gaussian kernel variance\n",
    "        self.sigma = sigma\n",
    "        # Variance of bin, where mu = 1 \n",
    "        self.exact_sigma = exact_sigma\n",
    "        # MLP architecture parameter\n",
    "        self.out_layers = out_layers\n",
    "        # Kernels layer\n",
    "        self.kernels = self._get_kernels_layers()\n",
    "        # MLP layer\n",
    "        self.mlp = self._get_mlp()\n",
    "        # Out activation function\n",
    "        self.out_activation = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def _kernels_params(\n",
    "        self,\n",
    "        n_kernels: int,\n",
    "        sigma: float,\n",
    "        exact_sigma: float,\n",
    "        lamb: Optional[float]=None,\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        list_mus = [1.0]\n",
    "        list_sigmas = [exact_sigma] # For exact match: small variance => exact match\n",
    "        if n_kernels == 1:\n",
    "            return (list_mus, list_sigmas)\n",
    "\n",
    "        # Bin size\n",
    "        bin_size = 2.0 / (n_kernels - 1)\n",
    "        # Add middle of the bin\n",
    "        list_mus.append(round(1 - bin_size / 2, 5))\n",
    "        # Different sigmas for different kernels\n",
    "        if lamb:\n",
    "            list_sigmas += [bin_size * lamb] * (n_kernels - 1)\n",
    "        else:\n",
    "            list_sigmas += [sigma] * (n_kernels - 1)\n",
    "        for i in range(1, n_kernels - 1):\n",
    "            mu = round(list_mus[i] - bin_size, 5)\n",
    "            list_mus.append(mu)\n",
    "        return (list(reversed(list_mus)), list(reversed(list_sigmas)))\n",
    "        \n",
    "\n",
    "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
    "        kernels = torch.nn.ModuleList()\n",
    "        mus, sigmas = self._kernels_params(self.kernel_num, self.sigma, self.exact_sigma)\n",
    "        kernels = []\n",
    "        for mu, sigma in zip(mus, sigmas):\n",
    "            kernels.append(GaussianKernel(mu, sigma))\n",
    "\n",
    "        kernels = torch.nn.ModuleList(kernels)\n",
    "        return kernels\n",
    "\n",
    "\n",
    "    def _get_mlp(self) -> torch.nn.Sequential:\n",
    "        if len(self.out_layers) == 0:\n",
    "            return torch.nn.Sequential(torch.nn.Linear(self.kernel_num, 1))\n",
    "\n",
    "        dims = [self.kernel_num, *self.out_layers, 1]\n",
    "        layers = []\n",
    "        for i in range(1, len(dims)):\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Linear(dims[i - 1], dims[i]))\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_1: Dict[str, torch.Tensor],\n",
    "            input_2: Dict[str, torch.Tensor]\n",
    "    ) -> torch.FloatTensor:\n",
    "        # Paiwise method\n",
    "        logits_1 = self.predict(input_1)\n",
    "        logits_2 = self.predict(input_2)\n",
    "\n",
    "        logits_diff = logits_1 - logits_2\n",
    "\n",
    "        out = self.out_activation(logits_diff)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
    "        # Broadcast to match query and doc lenght\n",
    "        query = self.embeddings(query).unsqueeze(2)\n",
    "        doc = self.embeddings(doc).unsqueeze(1)\n",
    "\n",
    "        return F.cosine_similarity(query, doc, dim=-1)\n",
    "\n",
    "\n",
    "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        KM = []\n",
    "        for kernel in self.kernels:\n",
    "            # shape = [B]\n",
    "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
    "            KM.append(K)\n",
    "\n",
    "        # shape = [B, K]\n",
    "        kernels_out = torch.stack(KM, dim=1)\n",
    "        return kernels_out\n",
    "\n",
    "\n",
    "    def predict(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
    "        # shape = [Batch, Left], [Batch, Right]\n",
    "        query, doc = inputs['query'], inputs['document']\n",
    "\n",
    "        # shape = [Batch, Left, Right]\n",
    "        matching_matrix = self._get_matching_matrix(query, doc)\n",
    "        # shape = [Batch, Kernels]\n",
    "        kernels_out = self._apply_kernels(matching_matrix)\n",
    "        # shape = [Batch]\n",
    "        out = self.mlp(kernels_out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Dataset`s classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, index_pairs_or_triplets: List[List[Union[str, float]]],\n",
    "                 idx_to_text_mapping: Dict[str, str], vocab: Dict[str, int], oov_val: int,\n",
    "                 preproc_func: Callable, max_len: int = 30):\n",
    "        self.index_pairs_or_triplets = index_pairs_or_triplets\n",
    "        self.idx_to_text_mapping = idx_to_text_mapping\n",
    "        self.vocab = vocab\n",
    "        self.oov_val = oov_val\n",
    "        self.preproc_func = preproc_func\n",
    "        self.max_len = max_len\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_pairs_or_triplets)\n",
    "\n",
    "\n",
    "    def _tokenized_text_to_index(self, tokenized_text: List[str]) -> List[int]:\n",
    "        list_of_indexes = []\n",
    "        for word in tokenized_text:\n",
    "            list_of_indexes.append(self.vocab.get(word, self.oov_val))\n",
    "        return list_of_indexes\n",
    "\n",
    "\n",
    "    def _convert_text_idx_to_token_idxs(self, idx: int) -> List[int]:\n",
    "        text = self.idx_to_text_mapping[str(idx)]\n",
    "        text = self.preproc_func(text)\n",
    "        return self._tokenized_text_to_index(text)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class TrainTripletsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        id_query, id_left, id_right, target = self.index_pairs_or_triplets[idx]\n",
    "        tokens_query = self._convert_text_idx_to_token_idxs(id_query)\n",
    "        tokens_left = self._convert_text_idx_to_token_idxs(id_left)\n",
    "        tokens_right = self._convert_text_idx_to_token_idxs(id_right)\n",
    "        sample_1 = {'query': tokens_query, 'document': tokens_left}\n",
    "        sample_2 = {'query': tokens_query, 'document': tokens_right}\n",
    "        \n",
    "        return (sample_1, sample_2, target)\n",
    "\n",
    "\n",
    "class ValPairsDataset(RankingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        id_left, id_right, target = self.index_pairs_or_triplets[idx]\n",
    "        tokens_left = self._convert_text_idx_to_token_idxs(id_left)\n",
    "        tokens_right = self._convert_text_idx_to_token_idxs(id_right)\n",
    "        sample = {'query': tokens_left, 'document': tokens_right}\n",
    "        \n",
    "        return (sample, target)\n",
    "\n",
    "\n",
    "def collate_fn(batch_objs: List[Union[Dict[str, torch.Tensor], torch.FloatTensor]]):\n",
    "    max_len_q1 = -1\n",
    "    max_len_d1 = -1\n",
    "    max_len_q2 = -1\n",
    "    max_len_d2 = -1\n",
    "\n",
    "    is_triplets = False\n",
    "    for elem in batch_objs:\n",
    "        if len(elem) == 3:\n",
    "            left_elem, right_elem, label = elem\n",
    "            is_triplets = True\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        max_len_q1 = max(len(left_elem['query']), max_len_q1)\n",
    "        max_len_d1 = max(len(left_elem['document']), max_len_d1)\n",
    "        if len(elem) == 3:\n",
    "            max_len_q2 = max(len(right_elem['query']), max_len_q2)\n",
    "            max_len_d2 = max(len(right_elem['document']), max_len_d2)\n",
    "\n",
    "    q1s = []\n",
    "    d1s = []\n",
    "    q2s = []\n",
    "    d2s = []\n",
    "    labels = []\n",
    "\n",
    "    for elem in batch_objs:\n",
    "        if is_triplets:\n",
    "            left_elem, right_elem, label = elem\n",
    "        else:\n",
    "            left_elem, label = elem\n",
    "\n",
    "        pad_len1 = max_len_q1 - len(left_elem['query'])\n",
    "        pad_len2 = max_len_d1 - len(left_elem['document'])\n",
    "        if is_triplets:\n",
    "            pad_len3 = max_len_q2 - len(right_elem['query'])\n",
    "            pad_len4 = max_len_d2 - len(right_elem['document'])\n",
    "\n",
    "        q1s.append(left_elem['query'] + [0] * pad_len1)\n",
    "        d1s.append(left_elem['document'] + [0] * pad_len2)\n",
    "        if is_triplets:\n",
    "            q2s.append(right_elem['query'] + [0] * pad_len3)\n",
    "            d2s.append(right_elem['document'] + [0] * pad_len4)\n",
    "        labels.append([label])\n",
    "    q1s = torch.LongTensor(q1s)\n",
    "    d1s = torch.LongTensor(d1s)\n",
    "    if is_triplets:\n",
    "        q2s = torch.LongTensor(q2s)\n",
    "        d2s = torch.LongTensor(d2s)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    ret_left = {'query': q1s, 'document': d1s}\n",
    "    if is_triplets:\n",
    "        ret_right = {'query': q2s, 'document': d2s}\n",
    "        return ret_left, ret_right, labels\n",
    "    else:\n",
    "        return ret_left, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = GLOVE_PATH\n",
    "\n",
    "\n",
    "embeddings = {}\n",
    "with open(file_path, encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.split(' ')\n",
    "        embeddings[line[0]] = line[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 400000, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings), len(set(embeddings)), len(list(embeddings.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def __init__(self, glue_qqp_dir: str, glove_vectors_path: str,\n",
    "                 min_token_occurancies: int = 1,\n",
    "                 random_seed: int = 0,\n",
    "                 emb_rand_uni_bound: float = 0.2,\n",
    "                 freeze_knrm_embeddings: bool = True,\n",
    "                 knrm_kernel_num: int = 21,\n",
    "                 knrm_out_mlp: List[int] = [],\n",
    "                 dataloader_bs: int = 1024,\n",
    "                 train_lr: float = 0.001,\n",
    "                 change_train_loader_ep: int = 10\n",
    "                 ):\n",
    "        # Data parameters\n",
    "        self.glue_qqp_dir = glue_qqp_dir\n",
    "        self.glove_vectors_path = glove_vectors_path\n",
    "        self.glue_train_df = self.get_glue_df('train')\n",
    "        self.glue_dev_df = self.get_glue_df('dev')\n",
    "        self.dev_pairs_for_ndcg = self.create_val_pairs(self.glue_dev_df)\n",
    "        # Minimal token occurancies not to be classified as \"low-frequency\"\n",
    "        self.min_token_occurancies = min_token_occurancies\n",
    "        # All\n",
    "        self.all_tokens = self.get_all_tokens(\n",
    "            [self.glue_train_df, self.glue_dev_df], self.min_token_occurancies)\n",
    "\n",
    "        self.random_seed = random_seed\n",
    "        self.emb_rand_uni_bound = emb_rand_uni_bound\n",
    "        self.freeze_knrm_embeddings = freeze_knrm_embeddings\n",
    "        self.knrm_kernel_num = knrm_kernel_num\n",
    "        self.knrm_out_mlp = knrm_out_mlp\n",
    "        self.dataloader_bs = dataloader_bs\n",
    "        self.train_lr = train_lr\n",
    "        self.change_train_loader_ep = change_train_loader_ep\n",
    "        # nDCG border\n",
    "        self.ndcg_border = 0.93\n",
    "\n",
    "        self.model, self.vocab, self.unk_words = self.build_knrm_model()\n",
    "        self.idx_to_text_mapping_train = self.get_idx_to_text_mapping(\n",
    "            self.glue_train_df)\n",
    "        self.idx_to_text_mapping_dev = self.get_idx_to_text_mapping(\n",
    "            self.glue_dev_df)\n",
    "\n",
    "        self.val_dataset = ValPairsDataset(self.dev_pairs_for_ndcg,\n",
    "              self.idx_to_text_mapping_dev,\n",
    "              vocab=self.vocab, oov_val=self.vocab['OOV'],\n",
    "              preproc_func=self.simple_preproc)\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(\n",
    "            self.val_dataset, batch_size=self.dataloader_bs, num_workers=0,\n",
    "            collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "\n",
    "    def get_glue_df(self, partition_type: str) -> pd.DataFrame:\n",
    "        \"\"\"Load glue DataFrame by `partition type`: ['dev', 'train']\n",
    "        and transform columns names:\n",
    "            * qid1 -> id_left;\n",
    "            * qid2 -> id_right;\n",
    "            * question1 -> text_left;\n",
    "            * question2 -> text_right;\n",
    "            * is_duplicate -> label.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        partition_type : `str`\n",
    "            Which part of data to load: ['dev', 'train'].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        glue_df_fin : `pandas.DataFrame`\n",
    "            DataFrame with transformed columns names.\n",
    "        \"\"\"\n",
    "        assert partition_type in ['dev', 'train']\n",
    "        glue_df = pd.read_csv(\n",
    "            self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', dtype=object)\n",
    "        glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "        glue_df_fin = pd.DataFrame({\n",
    "            'id_left'   : glue_df['qid1'],\n",
    "            'id_right'  : glue_df['qid2'],\n",
    "            'text_left' : glue_df['question1'],\n",
    "            'text_right': glue_df['question2'],\n",
    "            'label'     : glue_df['is_duplicate'].astype(int)\n",
    "        })\n",
    "        return glue_df_fin\n",
    "\n",
    "\n",
    "    def handle_punctuation(self, inp_str: str) -> str:\n",
    "        \"\"\"Предобработка:\n",
    "        Очистка строки от пунктуации.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inp_str : str\n",
    "            Input string to be preprocessed.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        inp_str : `str`\n",
    "            Same string w/o punctuation.\n",
    "        \"\"\"\n",
    "        for p in string.punctuation:\n",
    "            inp_str = inp_str.replace(p, '')\n",
    "        return inp_str\n",
    "\n",
    "\n",
    "    def simple_preproc(self, inp_str: str) -> List[str]:\n",
    "        \"\"\"Простая предобработка данных:\n",
    "        * Очистка от пунктуации;\n",
    "        * Очистка от пустных строк;\n",
    "        * Токенизация с помощью `nltk.word_tokenize`\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inp_str : str\n",
    "            Input string to be preprocessed.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list_of_tokenized_words : `List[str]`\n",
    "            List of tokenized words.\n",
    "        \"\"\"\n",
    "        inp_str = self.handle_punctuation(inp_str.lower())\n",
    "        inp_str = inp_str.strip()\n",
    "        return nltk.word_tokenize(inp_str)\n",
    "\n",
    "\n",
    "    def _filter_rare_words(self, vocab: Dict[str, int], min_occurancies: int) -> Dict[str, int]:\n",
    "        \"\"\"Filter rare words by `min_occurancies` of word in `vocab` {word: #_word_occurrences}\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab : `Dict[str, int]`\n",
    "            Vocabulary of words occurances.\n",
    "        min_occurancies : `int`\n",
    "            Minimal token occurancies not to be classified as \"low-frequency\".\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        vocab_filtered : `Dcit[str, int]`\n",
    "            Filtered vocabulary.\n",
    "        \"\"\"\n",
    "        return {word: cnt for word, cnt in vocab.items() if cnt >= min_occurancies}\n",
    "\n",
    "\n",
    "    def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "        \"\"\"Формирует список ВСЕХ токенов, представленных в подаваемых на вход датасетах (`pd.DataFrame`).\n",
    "        Алгоритм:\n",
    "            1. Сформировать уникальное множество всех текстов;\n",
    "            2. Рассчитать частотность каждого токена:\n",
    "                2.1. Препроцессинг с помощью `simple_preproc`;\n",
    "                2.2. Отсечь слова согласно порогу, равному `min_token_occurancies` \n",
    "                    с помощью метода `_filter_rare_words`. \n",
    "            3. Вернуть список токенов, для которых будут формироваться эмбеддинги \n",
    "                и на которые будут разбиваться оригинальные тексты вопросов.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        list_of_df : `pandas.DataFrame`\n",
    "            Список датафреймов с данными, из которых будут формироваться токены.\n",
    "        min_occurancies : `int`\n",
    "            Минимальное значение частоты появления токена.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        all_tokens : `List[str]`\n",
    "            Список всех предобработанных и отфильтрованных токенов.\n",
    "        \"\"\"\n",
    "        words_corpus = []\n",
    "        for df in list_of_df:\n",
    "            for col in ['text_left', 'text_right']:\n",
    "                for doc in df[col].values:\n",
    "                    words_corpus.append(doc)\n",
    "        words_corpus = ' '.join(set(words_corpus))\n",
    "        words_corpus = self.simple_preproc(words_corpus)\n",
    "        words_corpus = Counter(words_corpus)\n",
    "        words_corpus = self._filter_rare_words(words_corpus, min_occurancies)\n",
    "        return list(words_corpus)\n",
    "\n",
    "\n",
    "    def _read_glove_embeddings(self, file_path: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Reads GLOVE Embeddings from `file_path`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : `str`\n",
    "            Path to GLOVE Embeddings file.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embeddings : `Dict[str, List[str]]`\n",
    "        \"\"\"\n",
    "        embeddings = {}\n",
    "        with open(file_path, encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.split(' ')\n",
    "                embeddings[line[0]] = line[1:]\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def create_glove_emb_from_file(self,\n",
    "                                   file_path: str,\n",
    "                                   inner_keys: List[str],\n",
    "                                   random_seed: int,\n",
    "                                   rand_uni_bound: float\n",
    "            ) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
    "        \"\"\"Creates embedding matrix from Glove file \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file_path : `str`\n",
    "            Path to GLOVE Embeddings file.\n",
    "        inner_keys : `List[str]`\n",
    "            Unique list of keys tokens. \n",
    "        random_seed : `int`\n",
    "            Reseed the singleton RandomState instance.\n",
    "        rand_uni_bound : `float`\n",
    "            Uniform bounds for OutOfVocabulary word's embedding to be sampled by.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (emb_matix, vocab, unk_words) : `Tuple[np.ndarray, Dict[str, int], List[str]`\n",
    "\n",
    "        emb_matrix : `np.ndarray`\n",
    "            Embedding matrix w/ [N, D] shape, \n",
    "                where N - #_of_tokens,\n",
    "                      D - size of embedding vector.\n",
    "        vocab : `Dict[str, int]`\n",
    "            Vocabulary of tokens with 'PAD' and 'OOV' in 0, 1 indexes.\n",
    "        unk_words : `List[str]`\n",
    "            List of unknown words.\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        glove = self._read_glove_embeddings(file_path)\n",
    "        glove_tokens, inner_tokens = set(glove), set(inner_keys)\n",
    "        # List of unique unknown words\n",
    "        unk_words = ['PAD', 'OOV'] + list(inner_tokens.difference(glove_tokens))\n",
    "        # D = Embeddings size \n",
    "        dim = len(list(glove.values())[0])\n",
    "        # Embedding value sampled from U[+/-rand_uni_bound] for OutOfVocabulary words \n",
    "        oov = np.random.uniform(-rand_uni_bound, rand_uni_bound, dim)\n",
    "        # Create embadding matrix [N, D], N - #_of_tokens\n",
    "        emb_matrix = [np.zeros(dim), oov]\n",
    "\n",
    "        vocab = {\"PAD\": 0, \"OOV\": 1}\n",
    "        for i, token in enumerate(inner_tokens, start=2):\n",
    "            vocab[token] = i\n",
    "            embedding = glove.get(token, oov)\n",
    "            # Convert string embedding values from file to float\n",
    "            embedding = np.array(list(map(float, embedding)))\n",
    "            emb_matrix.append(embedding)\n",
    "        # [N, D] embedding matrix to np.ndarray\n",
    "        emb_matrix = np.array(emb_matrix)\n",
    "        return emb_matrix, vocab, unk_words\n",
    "\n",
    "\n",
    "    def build_knrm_model(self) -> Tuple[torch.nn.Module, Dict[str, int], List[str]]:\n",
    "        \"\"\"Builds KNRM model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        (knrm, vocab, unk_words) : `Tuple[torch.nn.Module, Dict[str, int], List[str]]`\n",
    "\n",
    "        knrm : torch.nn.Module\n",
    "            KNRM model class instance.\n",
    "        vocab : `Dict[str, int]`\n",
    "            Vocabulary of tokens with 'PAD' and 'OOV' in 0, 1 indexes.\n",
    "        unk_words : `List[str]`\n",
    "            List of unknown words.\n",
    "        \"\"\"\n",
    "        embedding_matrix, vocab, unk_words = self.create_glove_emb_from_file(\n",
    "            self.glove_vectors_path, self.all_tokens, self.random_seed, self.emb_rand_uni_bound)\n",
    "        \n",
    "        torch.manual_seed(self.random_seed)\n",
    "        knrm = KNRM(embedding_matrix=embedding_matrix,\n",
    "                    freeze_embeddings=self.freeze_knrm_embeddings,\n",
    "                    out_layers=self.knrm_out_mlp,\n",
    "                    kernel_num=self.knrm_kernel_num)\n",
    "        return knrm, vocab, unk_words\n",
    "\n",
    "\n",
    "    def _gen_pairs(self,\n",
    "                   list1: List[Union[str, float]],\n",
    "                   list2: List[Union[str, float]]):\n",
    "        return ((x, y) for x in list1 for y in list2)\n",
    "\n",
    "\n",
    "    def sample_data_for_train_iter(self, inp_df: pd.DataFrame, seed: int) -> List[List[Union[str, float]]]:\n",
    "        fill_top_to, min_group_size = -1, 2  # todo: vary\n",
    "\n",
    "        inp_df = inp_df[['id_left', 'id_right', 'label']]\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "\n",
    "        group_size = inp_df.groupby('id_left').size()\n",
    "        left_ind_to_use = group_size[group_size >= min_group_size].index.tolist()\n",
    "        groups = inp_df[inp_df['id_left'].isin(left_ind_to_use)].groupby('id_left')\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        out_pairs = []\n",
    "        for id_left, group in groups:\n",
    "            ones_ids = group[group.label == 1].id_right.values\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                cur_chosen = set(ones_ids).union(set(zeroes_ids)).union(set(id_left))\n",
    "                pad_sample = np.random.choice(list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            for doc1, doc2 in self._gen_pairs(ones_ids, zeroes_ids):\n",
    "                out_pairs.append([id_left, doc1, doc2, 1.0] if np.random.rand() > 0.5 else [id_left, doc2, doc1, 0.0])\n",
    "            for doc1, doc2 in self._gen_pairs(ones_ids, pad_sample):\n",
    "                out_pairs.append([id_left, doc1, doc2, 1.0] if np.random.rand() > 0.5 else [id_left, doc2, doc1, 0.0])\n",
    "            for doc1, doc2 in self._gen_pairs(zeroes_ids, pad_sample):\n",
    "                out_pairs.append([id_left, doc1, doc2, 1.0] if np.random.rand() > 0.5 else [id_left, doc2, doc1, 0.0])\n",
    "\n",
    "        return out_pairs\n",
    "\n",
    "    def create_val_pairs(self,\n",
    "                         inp_df: pd.DataFrame,\n",
    "                         fill_top_to: int = 15,\n",
    "                         min_group_size: int = 2,\n",
    "                         seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "        \"\"\"Creates val pairs for ValPairsDataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp_df : `pandas.DataFrame`\n",
    "            Input DataFrame.\n",
    "        fill_top_to : `int`\n",
    "            Top to fill.\n",
    "            Default = 15.\n",
    "        min_group_size : `int`\n",
    "            Min size of group.\n",
    "            Default = 2\n",
    "        seed : `int`\n",
    "            Random seed.\n",
    "            Default = 0.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out_pairs : `List[Union[str, float]]`\n",
    "            List of pairs to validation.\n",
    "        \"\"\"\n",
    "        # Select columns\n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        # Size of groups\n",
    "        inp_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "        # List of ids to use later from `id_left`\n",
    "        glue_dev_leftids_to_use = list(\n",
    "            inp_df_group_sizes[inp_df_group_sizes >= min_group_size].index)\n",
    "        # Groupby by `id_left``\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_dev_leftids_to_use)].groupby('id_left')\n",
    "        # Set of all ids from `id_left` and `id_right`\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "        # List of returned pairs\n",
    "        out_pairs = []\n",
    "        # Set RandomSeed\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        for id_left, group in groups:\n",
    "            ones_ids = group[group.label > 0].id_right.values\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                cur_chosen = set(ones_ids).union(\n",
    "                    set(zeroes_ids)).union({id_left})\n",
    "                pad_sample = np.random.choice(\n",
    "                    list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            for i in ones_ids:\n",
    "                out_pairs.append([id_left, i, 2])\n",
    "            for i in zeroes_ids:\n",
    "                out_pairs.append([id_left, i, 1])\n",
    "            for i in pad_sample:\n",
    "                out_pairs.append([id_left, i, 0])\n",
    "        return out_pairs\n",
    "\n",
    "\n",
    "    def get_idx_to_text_mapping(self, inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "        \"\"\"Creates a mapping between all ids (`id_left` or `id_right`) \n",
    "            and text from inputed dataframe.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp_df : `pandas.DataFrame`\n",
    "            Input DataFrame.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        left_dict : `Dict[str, str]`\n",
    "            Dictionary mapping id to text.\n",
    "        \"\"\"\n",
    "        left_dict = (\n",
    "            inp_df\n",
    "            [['id_left', 'text_left']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_left')\n",
    "            ['text_left']\n",
    "            .to_dict()\n",
    "        )\n",
    "        right_dict = (\n",
    "            inp_df\n",
    "            [['id_right', 'text_right']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_right')\n",
    "            ['text_right']\n",
    "            .to_dict()\n",
    "        )\n",
    "        left_dict.update(right_dict)\n",
    "        return left_dict\n",
    "\n",
    "\n",
    "    def ndcg_k(self,\n",
    "               y_true: np.ndarray,\n",
    "               y_pred: np.ndarray,\n",
    "               gain_scheme: str = 'exp2',\n",
    "               ndcg_top_k: int = 10) -> float:\n",
    "        \"\"\"Metric:\n",
    "            Calculates the Normalized Discounted Cumulative Gain (NDCG) \n",
    "            by k most relevant.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        ys_true : `np.ndarray`\n",
    "            Correct labels rank.\n",
    "        ys_pred : `np.ndarray`\n",
    "            Predicted labels rank.\n",
    "        gain_scheme : `str`\n",
    "            Gain scheme. Allowed values = ['const', 'exp2']\n",
    "                * const : gain = rank;\n",
    "                * exp2  : gain = 2^rank - 1.\n",
    "            Default = 'exp2'\n",
    "        ndcg_top_k : `int`\n",
    "            Top k most relevant objects.\n",
    "            Default = 10\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndcg_value : `float`\n",
    "            Metric value.\n",
    "        \"\"\"\n",
    "        def dcg_k(y_true: np.ndarray,\n",
    "                  y_pred: np.ndarray,\n",
    "                  gain_scheme: str,\n",
    "                  top_k: int)-> float:\n",
    "            indices = np.argsort(y_pred)[::-1]\n",
    "            ind = min((len(y_true), top_k))\n",
    "            y_true_sorted = y_true[indices][:ind]\n",
    "            gain = 0\n",
    "            for i, y in enumerate(y_true_sorted, start=1):\n",
    "                if gain_scheme == 'const':\n",
    "                    gain += y\n",
    "                elif gain_scheme == 'exp2':\n",
    "                    gain += (2 ** y - 1) / math.log2(i + 1)\n",
    "            return gain\n",
    "\n",
    "        empirical_dcg = dcg_k(y_true, y_pred, gain_scheme, ndcg_top_k)\n",
    "        ideal_dcg = dcg_k(y_true, y_true, gain_scheme, ndcg_top_k)\n",
    "        if ideal_dcg == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return empirical_dcg / ideal_dcg\n",
    "\n",
    "\n",
    "    def valid(self, model: torch.nn.Module, val_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        labels_and_groups = val_dataloader.dataset.index_pairs_or_triplets\n",
    "        labels_and_groups = pd.DataFrame(labels_and_groups, columns=['left_id', 'right_id', 'rel'])\n",
    "\n",
    "        all_preds = []\n",
    "        for batch in (val_dataloader):\n",
    "            inp_1, y = batch\n",
    "            preds = model.predict(inp_1)\n",
    "            preds_np = preds.detach().numpy()\n",
    "            all_preds.append(preds_np)\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        labels_and_groups['preds'] = all_preds\n",
    "\n",
    "        ndcgs = []\n",
    "        for cur_id in labels_and_groups.left_id.unique():\n",
    "            cur_df = labels_and_groups[labels_and_groups.left_id == cur_id]\n",
    "            ndcg = self.ndcg_k(cur_df.rel.values.reshape(-1), cur_df.preds.values.reshape(-1))\n",
    "            if np.isnan(ndcg):\n",
    "                ndcgs.append(0)\n",
    "            else:\n",
    "                ndcgs.append(ndcg)\n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "\n",
    "    def _get_train_loader(self, epoch_num: int):\n",
    "        triplets = self.sample_data_for_train_iter(self.glue_train_df, epoch_num)\n",
    "        train_dataset = TrainTripletsDataset(triplets,\n",
    "                                             self.idx_to_text_mapping_train,\n",
    "                                             vocab=self.vocab,\n",
    "                                             oov_val=self.vocab['OOV'],\n",
    "                                             preproc_func=self.simple_preproc)\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                       batch_size=self.dataloader_bs,\n",
    "                                                       num_workers=0,\n",
    "                                                       collate_fn=collate_fn,\n",
    "                                                       shuffle=True)\n",
    "        return train_dataloader\n",
    "\n",
    "\n",
    "    def train(self, n_epochs: int):\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=self.train_lr)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "\n",
    "        ndcg = 0\n",
    "        train_dataloader = self._get_train_loader(0)\n",
    "        for epoch in range(1, n_epochs+1):\n",
    "            cur_loss = 0\n",
    "            if epoch % self.change_train_loader_ep == 0:\n",
    "                train_dataloader = self._get_train_loader(epoch)\n",
    "\n",
    "            for batch in train_dataloader:\n",
    "                doc1, doc2, batch_true = batch\n",
    "                batch_pred = self.model(doc1, doc2)\n",
    "                loss = criterion(batch_pred, batch_true)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                cur_loss += loss.item()\n",
    "\n",
    "            if epoch % 2 == 0:\n",
    "                ndcg = self.valid(self.model, self.val_dataloader)\n",
    "\n",
    "            print(f'\\nEpoch: {epoch}/{n_epochs} --- loss: {round(cur_loss, 4)}\\tnDCG: {round(ndcg, 4)}')\n",
    "            if ndcg >= self.ndcg_border:\n",
    "                print('nDCG Border beated!')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/20 --- loss: 14.8223\tnDCG: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dcg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Solution(glue_qqp_dir\u001b[39m=\u001b[39mGLUE_QQP_DIR, glove_vectors_path\u001b[39m=\u001b[39mGLOVE_PATH)\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[11], line 532\u001b[0m, in \u001b[0;36mSolution.train\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m    529\u001b[0m     cur_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    531\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 532\u001b[0m     ndcg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_dataloader)\n\u001b[1;32m    534\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m --- loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(cur_loss,\u001b[39m \u001b[39m\u001b[39m4\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mnDCG: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mround\u001b[39m(ndcg,\u001b[39m \u001b[39m\u001b[39m4\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m ndcg \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndcg_border:\n",
      "Cell \u001b[0;32mIn[11], line 488\u001b[0m, in \u001b[0;36mSolution.valid\u001b[0;34m(self, model, val_dataloader)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mfor\u001b[39;00m cur_id \u001b[39min\u001b[39;00m labels_and_groups\u001b[39m.\u001b[39mleft_id\u001b[39m.\u001b[39munique():\n\u001b[1;32m    487\u001b[0m     cur_df \u001b[39m=\u001b[39m labels_and_groups[labels_and_groups\u001b[39m.\u001b[39mleft_id \u001b[39m==\u001b[39m cur_id]\n\u001b[0;32m--> 488\u001b[0m     ndcg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mndcg_k(cur_df\u001b[39m.\u001b[39;49mrel\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), cur_df\u001b[39m.\u001b[39;49mpreds\u001b[39m.\u001b[39;49mvalues\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m    489\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misnan(ndcg):\n\u001b[1;32m    490\u001b[0m         ndcgs\u001b[39m.\u001b[39mappend(\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 464\u001b[0m, in \u001b[0;36mSolution.ndcg_k\u001b[0;34m(self, y_true, y_pred, gain_scheme, ndcg_top_k)\u001b[0m\n\u001b[1;32m    461\u001b[0m             gain \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m y \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39mlog2(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    462\u001b[0m     \u001b[39mreturn\u001b[39;00m gain\n\u001b[0;32m--> 464\u001b[0m empirical_dcg \u001b[39m=\u001b[39m dcg(y_true, y_pred, gain_scheme, ndcg_top_k)\n\u001b[1;32m    465\u001b[0m ideal_dcg \u001b[39m=\u001b[39m dcg_k(y_true, y_true, gain_scheme, ndcg_top_k)\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m ideal_dcg \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dcg' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Solution(glue_qqp_dir=GLUE_QQP_DIR, glove_vectors_path=GLOVE_PATH)\n",
    "trainer.train(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solution:\n",
    "    def __init__(self,\n",
    "                 glue_qqp_dir: str,\n",
    "                 glove_vectors_path: str,\n",
    "                 min_token_occurancies: int = 1,\n",
    "                 random_seed: int = 0,\n",
    "                 emb_rand_uni_bound: float = 0.2,\n",
    "                 freeze_knrm_embeddings: bool = True,\n",
    "                 knrm_kernel_num: int = 21,\n",
    "                 knrm_out_mlp: List[int] = [],\n",
    "                 dataloader_bs: int = 1024,\n",
    "                 train_lr: float = 0.001,\n",
    "                 change_train_loader_ep: int = 10\n",
    "                 ):\n",
    "        self.glue_qqp_dir = glue_qqp_dir\n",
    "        self.glove_vectors_path = glove_vectors_path\n",
    "        self.glue_train_df = self.get_glue_df('train')\n",
    "        self.glue_dev_df = self.get_glue_df('dev')\n",
    "        self.dev_pairs_for_ndcg = self.create_val_pairs(self.glue_dev_df)\n",
    "        self.min_token_occurancies = min_token_occurancies\n",
    "        self.all_tokens = self.get_all_tokens([self.glue_train_df, self.glue_dev_df],\n",
    "                                              self.min_token_occurancies,\n",
    "                                              )\n",
    "        self.random_seed = random_seed\n",
    "        self.emb_rand_uni_bound = emb_rand_uni_bound\n",
    "        self.freeze_knrm_embeddings = freeze_knrm_embeddings\n",
    "        self.knrm_kernel_num = knrm_kernel_num\n",
    "        self.knrm_out_mlp = knrm_out_mlp\n",
    "        self.dataloader_bs = dataloader_bs\n",
    "        self.train_lr = train_lr\n",
    "        self.change_train_loader_ep = change_train_loader_ep\n",
    "        # nDCG border\n",
    "        self.ndcg_border = 0.93\n",
    "\n",
    "        self.model, self.vocab, self.unk_words = self.build_knrm_model()\n",
    "        self.idx_to_text_mapping_train = self.get_idx_to_text_mapping(\n",
    "            self.glue_train_df)\n",
    "        self.idx_to_text_mapping_dev = self.get_idx_to_text_mapping(\n",
    "            self.glue_dev_df)\n",
    "\n",
    "        self.val_dataset = ValPairsDataset(self.dev_pairs_for_ndcg,\n",
    "                                           self.idx_to_text_mapping_dev,\n",
    "                                           vocab=self.vocab,\n",
    "                                           oov_val=self.vocab['OOV'],\n",
    "                                           preproc_func=self.simple_preproc,\n",
    "                                           )\n",
    "        self.val_dataloader = torch.utils.data.DataLoader(self.val_dataset,\n",
    "                                                          batch_size=self.dataloader_bs,\n",
    "                                                          num_workers=0,\n",
    "                                                          collate_fn=collate_fn,\n",
    "                                                          shuffle=False,\n",
    "                                                          )\n",
    "\n",
    "    def get_glue_df(self, partition_type: str) -> pd.DataFrame:\n",
    "        assert partition_type in ['dev', 'train']\n",
    "        glue_df = pd.read_csv(self.glue_qqp_dir + f'/{partition_type}.tsv', \n",
    "                              header=0, delimiter=\"\\t\", \n",
    "                              quoting=csv.QUOTE_NONE, encoding='utf-8', \n",
    "                              index_col=[0], dtype=object)\n",
    "        glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "        glue_df_fin = pd.DataFrame({\n",
    "            'id_left': glue_df['qid1'],\n",
    "            'id_right': glue_df['qid2'],\n",
    "            'text_left': glue_df['question1'],\n",
    "            'text_right': glue_df['question2'],\n",
    "            'label': glue_df['is_duplicate'].astype(int)\n",
    "        })\n",
    "        return glue_df_fin\n",
    "\n",
    "    def hadle_punctuation(self, inp_str: str) -> str:\n",
    "        for punct in string.punctuation:\n",
    "            inp_str = inp_str.replace(punct, ' ')\n",
    "        return inp_str\n",
    "\n",
    "    def simple_preproc(self, inp_str: str) -> List[str]:\n",
    "        inp_str = self.hadle_punctuation(inp_str.lower())\n",
    "        inp_str = inp_str.strip()\n",
    "        return nltk.word_tokenize(inp_str)\n",
    "\n",
    "    def _filter_rare_words(self, vocab: Dict[str, int], min_occurancies: int) -> Dict[str, int]:\n",
    "        return {k: c for k, c in vocab.items() if c >= min_occurancies}\n",
    "\n",
    "    def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
    "        corpus = []\n",
    "        for df in list_of_df:\n",
    "            for col in ['text_left', 'text_right']:\n",
    "                for doc in df[col].values:\n",
    "                    corpus.append(doc)\n",
    "        corpus = ' '.join(set(corpus))\n",
    "        corpus = self.simple_preproc(corpus)\n",
    "        corpus = Counter(corpus)\n",
    "        corpus = self._filter_rare_words(corpus, min_occurancies)\n",
    "        return list(corpus)\n",
    "\n",
    "    def _read_glove_embeddings(self, file_path: str) -> Dict[str, List[str]]:\n",
    "        embed = {}\n",
    "        with open(file_path, encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                line = line.split(' ')\n",
    "                embed[line[0]] = line[1:]\n",
    "\n",
    "        return embed\n",
    "\n",
    "    def create_glove_emb_from_file(self, file_path: str, inner_keys: List[str],\n",
    "                                   random_seed: int, rand_uni_bound: float\n",
    "                                   ) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        glove = self._read_glove_embeddings(file_path)\n",
    "        glove_tokens, inner_tokens = set(glove), set(inner_keys)\n",
    "\n",
    "        unk_words = ['PAD', 'OOV'] + list(inner_tokens.difference(glove_tokens))\n",
    "\n",
    "        dim = len(list(glove.values())[0])\n",
    "        oov = np.random.uniform(-rand_uni_bound, rand_uni_bound, dim)\n",
    "        emb_matrix = [np.zeros(dim), oov]\n",
    "\n",
    "        vocab = {'PAD': 0, 'OOV': 1}\n",
    "        for i, token in enumerate(inner_tokens, start=2):\n",
    "            vocab[token] = i\n",
    "            vect = glove.get(token, oov)\n",
    "            vect = np.array(list(map(float, vect)))\n",
    "            emb_matrix.append(vect)\n",
    "\n",
    "        emb_matrix = np.array(emb_matrix)\n",
    "        return emb_matrix, vocab, unk_words\n",
    "\n",
    "    def build_knrm_model(self) -> Tuple[torch.nn.Module, Dict[str, int], List[str]]:\n",
    "        emb_matrix, vocab, unk_words = self.create_glove_emb_from_file(self.glove_vectors_path,\n",
    "                                                                       self.all_tokens,\n",
    "                                                                       self.random_seed,\n",
    "                                                                       self.emb_rand_uni_bound,\n",
    "                                                                       )\n",
    "        torch.manual_seed(self.random_seed)\n",
    "        knrm = KNRM(emb_matrix,\n",
    "                    freeze_embeddings=self.freeze_knrm_embeddings,\n",
    "                    out_layers=self.knrm_out_mlp,\n",
    "                    kernel_num=self.knrm_kernel_num,\n",
    "                    )\n",
    "        return knrm, vocab, unk_words\n",
    "\n",
    "    def _gen_pairs(self, list1, list2):\n",
    "        return ((x, y) for x in list1 for y in list2)\n",
    "\n",
    "    def sample_data_for_train_iter(self, inp_df: pd.DataFrame, seed: int) -> List[List[Union[str, float]]]:\n",
    "        fill_top_to, min_group_size = -1, 2  # todo: vary\n",
    "\n",
    "        inp_df = inp_df[['id_left', 'id_right', 'label']]\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "\n",
    "        group_size = inp_df.groupby('id_left').size()\n",
    "        left_ind_to_use = group_size[group_size >= min_group_size].index.tolist()\n",
    "        groups = inp_df[inp_df['id_left'].isin(left_ind_to_use)].groupby('id_left')\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        out_pairs = []\n",
    "        for id_left, group in groups:\n",
    "            ones_ids = group[group.label == 1].id_right.values\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                cur_chosen = set(ones_ids).union(set(zeroes_ids)).union(set(id_left))\n",
    "                pad_sample = np.random.choice(list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            for doc1, doc2 in self._gen_pairs(ones_ids, zeroes_ids):\n",
    "                out_pairs.append([id_left, doc1, doc2, 1.0] if np.random.rand() > 0.5 else [id_left, doc2, doc1, 0.0])\n",
    "            for doc1, doc2 in self._gen_pairs(ones_ids, pad_sample):\n",
    "                out_pairs.append([id_left, doc1, doc2, 1.0] if np.random.rand() > 0.5 else [id_left, doc2, doc1, 0.0])\n",
    "            for doc1, doc2 in self._gen_pairs(zeroes_ids, pad_sample):\n",
    "                out_pairs.append([id_left, doc1, doc2, 1.0] if np.random.rand() > 0.5 else [id_left, doc2, doc1, 0.0])\n",
    "\n",
    "        return out_pairs\n",
    "\n",
    "    def create_val_pairs(self, inp_df: pd.DataFrame, fill_top_to: int = 15,\n",
    "                         min_group_size: int = 2, seed: int = 0) -> List[List[Union[str, float]]]:\n",
    "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
    "        inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
    "        glue_dev_leftids_to_use = list(\n",
    "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
    "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
    "            glue_dev_leftids_to_use)].groupby('id_left')\n",
    "\n",
    "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
    "\n",
    "        out_pairs = []\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        for id_left, group in groups:\n",
    "            ones_ids = group[group.label > 0].id_right.values\n",
    "            zeroes_ids = group[group.label == 0].id_right.values\n",
    "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
    "            num_pad_items = max(0, fill_top_to - sum_len)\n",
    "            if num_pad_items > 0:\n",
    "                cur_chosen = set(ones_ids).union(\n",
    "                    set(zeroes_ids)).union({id_left})\n",
    "                pad_sample = np.random.choice(\n",
    "                    list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
    "            else:\n",
    "                pad_sample = []\n",
    "            for i in ones_ids:\n",
    "                out_pairs.append([id_left, i, 2])\n",
    "            for i in zeroes_ids:\n",
    "                out_pairs.append([id_left, i, 1])\n",
    "            for i in pad_sample:\n",
    "                out_pairs.append([id_left, i, 0])\n",
    "        return out_pairs\n",
    "\n",
    "    def get_idx_to_text_mapping(self, inp_df: pd.DataFrame) -> Dict[str, str]:\n",
    "        left_dict = (\n",
    "            inp_df\n",
    "            [['id_left', 'text_left']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_left')\n",
    "            ['text_left']\n",
    "            .to_dict()\n",
    "        )\n",
    "        right_dict = (\n",
    "            inp_df\n",
    "            [['id_right', 'text_right']]\n",
    "            .drop_duplicates()\n",
    "            .set_index('id_right')\n",
    "            ['text_right']\n",
    "            .to_dict()\n",
    "        )\n",
    "        left_dict.update(right_dict)\n",
    "        return left_dict\n",
    "\n",
    "    def _dcg_k(self, ys_true: np.array, ys_pred: np.array, top_k: int) -> float:\n",
    "        indices = np.argsort(ys_pred)[::-1]\n",
    "        ind = min((len(ys_true), top_k))\n",
    "        ys_true_sorted = ys_true[indices][:ind]\n",
    "        gain = 0\n",
    "        for i, y in enumerate(ys_true_sorted, start=1):\n",
    "            gain += (2 ** y - 1) / math.log2(i + 1)\n",
    "\n",
    "        return gain\n",
    "\n",
    "    def ndcg_k(self, ys_true: np.array, ys_pred: np.array, ndcg_top_k: int = 10) -> float:\n",
    "        ideal_dcg = self._dcg_k(ys_true, ys_true, ndcg_top_k)\n",
    "        dcg = self._dcg_k(ys_true, ys_pred, ndcg_top_k)\n",
    "        if ideal_dcg == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return dcg / ideal_dcg\n",
    "\n",
    "    def valid(self, model: torch.nn.Module, val_dataloader: torch.utils.data.DataLoader) -> float:\n",
    "        labels_and_groups = val_dataloader.dataset.index_pairs_or_triplets\n",
    "        labels_and_groups = pd.DataFrame(labels_and_groups, columns=['left_id', 'right_id', 'rel'])\n",
    "\n",
    "        all_preds = []\n",
    "        for batch in val_dataloader:\n",
    "            inp_1, y = batch\n",
    "            preds = model.predict(inp_1)\n",
    "            preds_np = preds.detach().numpy()\n",
    "            all_preds.append(preds_np)\n",
    "        all_preds = np.concatenate(all_preds, axis=0)\n",
    "        labels_and_groups['preds'] = all_preds\n",
    "\n",
    "        ndcgs = []\n",
    "        for cur_id in labels_and_groups.left_id.unique():\n",
    "            cur_df = labels_and_groups[labels_and_groups.left_id == cur_id]\n",
    "            ndcg = self.ndcg_k(cur_df.rel.values.reshape(-1), cur_df.preds.values.reshape(-1))\n",
    "            if np.isnan(ndcg):\n",
    "                ndcgs.append(0)\n",
    "            else:\n",
    "                ndcgs.append(ndcg)\n",
    "\n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "    def _get_train_loader(self, epoch_num):\n",
    "        triplets = self.sample_data_for_train_iter(self.glue_train_df, epoch_num)\n",
    "        train_dataset = TrainTripletsDataset(triplets,\n",
    "                                             self.idx_to_text_mapping_train,\n",
    "                                             vocab=self.vocab,\n",
    "                                             oov_val=self.vocab['OOV'],\n",
    "                                             preproc_func=self.simple_preproc,\n",
    "                                             )\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                       batch_size=self.dataloader_bs,\n",
    "                                                       num_workers=0,\n",
    "                                                       collate_fn=collate_fn,\n",
    "                                                       shuffle=True,\n",
    "                                                       )\n",
    "        return train_dataloader\n",
    "\n",
    "    def train(self, n_epochs: int):\n",
    "        opt = torch.optim.SGD(self.model.parameters(), lr=self.train_lr)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "\n",
    "        ndcg = 0\n",
    "        train_dataloader = self._get_train_loader(0)\n",
    "        for epoch in range(1, n_epochs+1):\n",
    "            cur_loss = 0\n",
    "            if epoch % self.change_train_loader_ep == 0:\n",
    "                train_dataloader = self._get_train_loader(epoch)\n",
    "\n",
    "            for batch in train_dataloader:\n",
    "                doc1, doc2, batch_true = batch\n",
    "                batch_pred = self.model(doc1, doc2)\n",
    "                loss = criterion(batch_pred, batch_true)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                cur_loss += loss.item()\n",
    "\n",
    "            if epoch % 2 == 0:\n",
    "                ndcg = self.valid(self.model, self.val_dataloader)\n",
    "\n",
    "            print(f'\\nEpoch: {epoch}/{n_epochs} --- loss: {round(cur_loss, 4)}\\tnDCG: {round(ndcg, 4)}')\n",
    "            if ndcg >= self.ndcg_border:\n",
    "                print('nDCG Border beated!')\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
